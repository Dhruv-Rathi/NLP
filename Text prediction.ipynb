{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_CS60075_A21_Assn1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z_wN2v1RT1F"
      },
      "source": [
        "# **Assignment-1 for CS60075: Natural Language Processing**\n",
        "\n",
        "#### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\n",
        "\n",
        "#### Date of Announcement: 4th Sept, 2021\n",
        "#### Deadline for Submission: 11.59pm on Sunday, 12th Sept, 2021 \n",
        "\n",
        "#### (**NOTE**: Submit a .zip file, containing this .ipynb file, named as `<Your_Roll_Number>_Assn1_NLP_A21.ipynb` and the raw text corpus named `<Your_Roll_Number>_Assn1_rawCorpus.txt`. For example, if your roll number is 20XX12Y45, name the .ipynb file as `20XX12Y45_Assn1_NLP_A21.ipynb`. Name the .zip as `<Your_Roll_Number>_Assn1_NLP_A21.zip`. Write your code in the respective designated portion of the .ipynb. Also before submitting, make sure that all the outputs of your code are present in the .ipynb file itself.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a35tmEySCx7"
      },
      "source": [
        "### **Submission Details:**\n",
        "Name:\n",
        "\n",
        "Roll No.:\n",
        "\n",
        "Department:\n",
        "\n",
        "Email-ID:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9weHMmyd8fnq"
      },
      "source": [
        "## **Reading a Raw Text Corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmSy_LOK2aGQ"
      },
      "source": [
        "Retrieve & save raw corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rku6rV2ORpZA"
      },
      "source": [
        "# To construct your corpus, retrieve (through Python code) Chapter I to Chapter X,\n",
        "# both inclusive, from the link below:\n",
        "# \"https://www.gutenberg.org/files/730/730-0.txt\"\n",
        "# Save this corpus in a text file, named as 'rawCorpus.txt'\n",
        "# Print the total number of characters in the text file \n",
        "\n",
        "# *** Write code ***"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KZIOy0Y2hzQ"
      },
      "source": [
        "Read the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsdBJa_l2l7g"
      },
      "source": [
        "# Read the corpus from rawCorpus.txt, in a variable `rawReadCorpus`\n",
        "# *** Write code ***\n",
        "\n",
        "print (\"Total # of characters in read dataset: {}\".format(len(rawReadCorpus)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhkmGsSoV0zG"
      },
      "source": [
        "## **Installing NLTK**\n",
        "\n",
        "The Natural Language Toolkit ([NLTK](https://www.nltk.org/)) is a Python module that is intended to support research and teaching in NLP or closely related areas. \n",
        "\n",
        "Detailed installation instructions to install NLTK can be found at this [link](https://www.nltk.org/install.html).\n",
        "\n",
        "To ensure uniformity, we suggest to use **python3**. You can download Anaconda3 and create a separate environment to do this assignment, eg.\n",
        "```bash\n",
        "conda create -n myenv python=3.6\n",
        "conda activate myenv\n",
        "```\n",
        "\n",
        "The link to anaconda3 for Windows and Linux is available here https://docs.anaconda.com/anaconda/install/. Subsequently, you can install NLTK through the following commands:\n",
        "```bash\n",
        "sudo pip3 install nltk \n",
        "python3 \n",
        "nltk.download()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utKtZeHq4N98"
      },
      "source": [
        "## **Preprocessing the corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-LSUX__82Ff"
      },
      "source": [
        "**Tokenize into words and sentences, using NLTK library:** Using the NLTK modules imported above, retrieve a case-insensitive preprocessed model. Make sure to take care of words like \"\\_will\\_\" (that should ideally appear as \"will\"), \"wouldn't\" (that should ideally appear as a single word, and not multiple tokens) and other occurences of special cases that you find in the raw corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g7eO4Dm4jIn"
      },
      "source": [
        "# Importing modules\n",
        "import nltk\n",
        "nltk.download('punkt') # For tokenizers\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWIzYXyz9Zt_"
      },
      "source": [
        "# *** Write code for preprocessing the corpus ***\n",
        "# Print first 5 sentences of your preprocessed corpus *** Write code ***\n",
        "# Print first 5 words/tokens of your preprocessed corpus *** Write code ***"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ75_a1QL70J"
      },
      "source": [
        "**Perform the following tasks for the given corpus:**\n",
        "1. Print the average number of tokens per sentence.\n",
        "2. Print the length of the longest and the shortest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive).\n",
        "3. Print the number of unique tokens in the corpus, after stopword removal using the stopwords from NLTK (case-insensitive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyG0g3oSADmV"
      },
      "source": [
        "# Importing modules\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydHIxC7lG7Py"
      },
      "source": [
        "# *** Write code for the 2 tasks above ***"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RiDR7TJjKX"
      },
      "source": [
        "## **Language Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJeTSt8HM95L"
      },
      "source": [
        "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
        "\n",
        "1. **Create the following language models** on the given corpus: <br>\n",
        "    i.   Unigram <br>\n",
        "    ii.  Bigram <br>\n",
        "    iii. Trigram <br>\n",
        "\n",
        "2. **List the top 10 bigrams, trigrams**\n",
        "(Additionally remove those items which contain only articles, prepositions, determiners eg. \"of the\", \"in a\", etc. List top-10 bigrams/trigrams in both the original and processed models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlPXGvVaR-ka"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "unigrams=[]\n",
        "bigrams=[]\n",
        "trigrams=[]\n",
        "\n",
        "for content in (#pre_processed corpus): # *** Write code ***\n",
        "    unigrams.extend(words)\n",
        "    bigrams.extend(ngrams(content,2))\n",
        "    ##similar for trigrams \n",
        "    # *** Write code ***\n",
        "\n",
        "print (\"Sample of n-grams:\\n\" + \"-------------------------\")\n",
        "print (\"--> UNIGRAMS: \\n\" + str(unigrams[:5]) + \" ...\\n\")\n",
        "print (\"--> BIGRAMS: \\n\" + str(bigrams[:5]) + \" ...\\n\")\n",
        "print (\"--> TRIGRAMS: \\n\" + str(trigrams[:5]) + \" ...\\n\")\n",
        "\n",
        "# list of unigram, bigram & trigram after removing those that \n",
        "# totally contain only articles, prepositions, determiners\n",
        "# Eg. For bigrams, don't remove items like (\"a\", \"boy\") --> where not all are \n",
        "#     articles, prepositions, determiners\n",
        "#     But remove items like (\"in\", \"the\") --> where all are articles, prepositions, determiners\n",
        "# Similarly, for unigrams and trigrams\n",
        "unigrams_Processed = # *** Write code ***\n",
        "bigrams_Processed = # *** Write code ***\n",
        "trigrams_Processed = # *** Write code ***\n",
        "\n",
        "print (\"Sample of n-grams after processing:\\n\" + \"-------------------------\")\n",
        "print (\"--> UNIGRAMS: \\n\" + str(unigrams_Processed[:5]) + \" ...\\n\")\n",
        "print (\"--> BIGRAMS: \\n\" + str(bigrams_Processed[:5]) + \" ...\\n\")\n",
        "print (\"--> TRIGRAMS: \\n\" + str(trigrams_Processed[:5]) + \" ...\\n\")\n",
        "\n",
        "def get_ngrams_freqDist(n, ngramList):\n",
        "    #This function computes the frequency corresponding to each ngram in ngramList \n",
        "    #Here, n=1 for unigram, n=2 for bigram, etc.\n",
        "    #ngramList = list of unigrams when n=1, ngramList = list of bigrams when n=2\n",
        "    #Returns: ngram_freq_dict (a Python dictionary where key = a ngram, value = its frequency)\n",
        "    \n",
        "    # *** Write code ***\n",
        "    \n",
        "    return ngram_freq_dict\n",
        "\n",
        "unigrams_freqDist = get_ngrams_freqDist(1, unigrams)\n",
        "unigrams_Processed_freqDist = get_ngrams_freqDist(1, unigrams_Processed)\n",
        "bigrams_freqDist = get_ngrams_freqDist(2, bigrams_Processed)\n",
        "bigrams_Processed_freqDist = get_ngrams_freqDist(2, bigrams_Processed)\n",
        "trigrams_freqDist = get_ngrams_freqDist(3, trigrams_Processed)\n",
        "trigrams_Processed_freqDist = get_ngrams_freqDist(3, trigrams_Processed)                                                 \n",
        "\n",
        "# Print top 10 unigrams, having highest frequency as in unigrams_freqDist\n",
        "# *** Write code ***\n",
        "\n",
        "# Print top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist\n",
        "# *** Write code ***\n",
        "\n",
        "# Print top 10 bigrams, having highest frequency as in bigrams_freqDist\n",
        "# *** Write code ***\n",
        "\n",
        "# Print top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist\n",
        "# *** Write code ***\n",
        "\n",
        "# Print top 10 trigrams, having highest frequency as in trigrams_freqDist\n",
        "# *** Write code ***\n",
        "\n",
        "# Print top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist\n",
        "# *** Write code ***"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqu8nVV7NREo"
      },
      "source": [
        "## **Next three words' Prediction using Smoothed Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2vnIM26b2WA"
      },
      "source": [
        "For a bigram model, add-one smoothing is defined by $P_{Add-1}(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+V}$.\n",
        "That is, pretend we saw each word one more time than we did.\n",
        "\n",
        "You have two tasks here.\n",
        "\n",
        "First, compute the smoothed bigram and trigram models from the bigrams_freqDist and trigrams_freqDist you calculated above (use the unprocessed models). Second, using these smoothed models, predict the next 3 possible word sequences for testSent1, testSent2 and testSent3, using your smoothed models.\n",
        "\n",
        "As for example, for the string 'Raj has a' the answers can be as below: \n",
        "\n",
        "(1) Raj has a **beautiful red car**\n",
        "\n",
        "(2) Raj has a **charismatic magnetic personality**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAGB1_S8NThy"
      },
      "source": [
        "testSent1 = \"There was a sudden jerk, a terrific convulsion of the limbs; and there he\"\n",
        "testSent2 = \"They made room for the stranger, but he sat down\"\n",
        "testSent3 = \"The hungry and destitute situation of the infant orphan was duly reported by\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLY1ymH-ZuJu"
      },
      "source": [
        "# *** Write code ***"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxfeaacTdO6h"
      },
      "source": [
        "Check the presence of these sentences in the original corpus at https://www.gutenberg.org/files/730/730-0.txt . How did your smoothed models perform in comparison to the original sentences? Compare them below.\n",
        "\n",
        "Did you notice something special about testSent3, in comparison to testSent1 and testSent2? If yes, what is it? Can you explain it?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFMkW9hKecxK"
      },
      "source": [
        "  - - - - - - - - - -\n",
        "  ** Answer here ** \n",
        "   - - - - - - - - - -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVBMcaAJXR9S"
      },
      "source": [
        "Which of the three models you generated above (unigram, bigram, trigram) is better in terms of **perplexity**, for the three test sentences (unseen data)? Write a piece of code to justify your answer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAPa1OVZX8uN"
      },
      "source": [
        "  - - - - - - - - - -\n",
        "  ** Answer here ** \n",
        "   - - - - - - - - - -"
      ]
    }
  ]
}