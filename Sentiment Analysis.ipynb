{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "NLP_CS60075_A21_Assn2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit"
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment-2 for CS60075: Natural Language Processing**\n",
        "\n",
        "#### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\n",
        "\n",
        "#### Date of Announcement: 15th Sept, 2021\n",
        "#### Deadline for Submission: 11.59pm on Wednesday, 22nd Sept, 2021 \n",
        "#### Submit this .ipynb file, named as `<Your_Roll_Number>_Assn2_NLP_A21.ipynb`"
      ],
      "metadata": {
        "id": "zyJ25uz0kSaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The central idea of this assignment is to use Naive Bayes classifier and LSTM based classifier and compare the models by accuracy on IMDB dataset.  This dataset consists of 50k movie reviews (25k positive, 25k negative). You can download the dataset from https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n"
      ],
      "metadata": {
        "id": "Ao1nhg9RknmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please submit with outputs. "
      ],
      "metadata": {
        "id": "ONM5Q4SCe9Mr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from sklearn.metrics import classification_report\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /home/dhruv04/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/dhruv04/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/dhruv04/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "metadata": {
        "id": "ElRkQElWUMjG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "#Load the IMDB dataset. You can load it using pandas as dataframe\n",
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "rev = df['review']\n",
        "sent = df['sentiment']"
      ],
      "outputs": [],
      "metadata": {
        "id": "fhHRim2AUm4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "PrePrecessing that needs to be done on lower cased corpus\n",
        "\n",
        "1. Remove html tags\n",
        "2. Remove URLS\n",
        "3. Remove non alphanumeric character\n",
        "4. Remove Stopwords\n",
        "5. Perform stemming and lemmatization\n",
        "\n",
        "You can use regex from re. "
      ],
      "metadata": {
        "id": "lK_Hn2f6VMP7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "rev1 = []\n",
        "\n",
        "# Removing html tags and URLs\n",
        "for i in range(len(rev)):\n",
        "    soup = BeautifulSoup(rev[i], 'html.parser')\n",
        "    for a in soup.findAll('a', href=True):\n",
        "        a.extract()\n",
        "    text = soup.get_text()\n",
        "    rev1.append(text)\n",
        "\n",
        "words = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "for i in range(len(rev1)):\n",
        "    words.append(nltk.word_tokenize(rev1[i]))\n",
        "    words[i] = [word.lower() for word in words[i] if word.isalnum()] #Removing non alphanumeric characters\n",
        "    words[i] = [word.lower() for word in words[i] if word not in stop_words] #Removing Stopwords\n",
        "    # print(len(words[i]))\n",
        "\n",
        "\n",
        "# #Function for stemming each review\n",
        "# def stemSentence(sentence):\n",
        "#     stem_sentence=\"\"\n",
        "#     for word in sentence:\n",
        "#         stem_sentence = ' ' + (porter.stem(word))\n",
        "#     return stem_sentence\n",
        "\n",
        "# #Function for Lematizing each review\n",
        "# def lemSentence(sentence):\n",
        "#     lem_sentence = \"\"\n",
        "#     for word in sentence:\n",
        "#         lem_sentence = lem_sentence+' '+(lemmatizer.lemmatize(word))\n",
        "#     return lem_sentence\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "5B5lHZPsVOXv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# Performing Stemming\n",
        "\n",
        "porter = PorterStemmer()\n",
        "words2 = []\n",
        "for i in range(len(words)):\n",
        "    tstr = \"\"\n",
        "    for j in range(len(words[i])):\n",
        "        words[i][j] = porter.stem(words[i][j])  \n",
        "        tstr = tstr + ' ' + words[i][j]\n",
        "    words2.append(tstr)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# Print Statistics of Data like avg length of sentence , proposition of data w.r.t class labels\n",
        "avg = 0\n",
        "pos = 0\n",
        "neg = 0\n",
        "Max = 0\n",
        "for i in range(len(words)):\n",
        "    avg+=len(words[i])\n",
        "    if(sent[i] == 'positive'):\n",
        "        pos+=1\n",
        "    else:\n",
        "        neg+=1\n",
        "    if(len(words[i]) > Max):\n",
        "        Max = len(words[i])\n",
        "avg = avg/len(words)\n",
        "print(\"Average length of sentence: \", avg)\n",
        "print(\"Positive reviews: \", pos)\n",
        "print(\"Negative reviews: \", neg)\n",
        "print(\"Maximum Length of sentences: \", Max)\n",
        "print(\"Reviews after stemming:\")\n",
        "print(words2[:5])\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average length of sentence:  113.89472\n",
            "Positive reviews:  25000\n",
            "Negative reviews:  25000\n",
            "Maximum Length of sentences:  1404\n",
            "Reviews after stemming:\n",
            "[' one review mention watch 1 oz episod hook right exactli happen first thing struck oz brutal unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home mani aryan muslim gangsta latino christian italian irish scuffl death stare dodgi deal shadi agreement never far would say main appeal show due fact goe show would dare forget pretti pictur paint mainstream audienc forget charm forget romanc oz mess around first episod ever saw struck nasti surreal could say readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard sold nickel inmat kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort view that get touch darker side', ' wonder littl product film techniqu fashion give comfort sometim discomfort sens realism entir piec actor extrem well michael sheen got polari voic pat truli see seamless edit guid refer william diari entri well worth watch terrificli written perform piec master product one great master comedi life realism realli come home littl thing fantasi guard rather use tradit techniqu remain solid disappear play knowledg sens particularli scene concern orton halliwel set particularli flat halliwel mural decor everi surfac terribl well done', ' thought wonder way spend time hot summer weekend sit air condit theater watch comedi plot simplist dialogu witti charact likabl even well bread suspect serial killer may disappoint realiz match point 2 risk addict thought proof woodi allen still fulli control style mani us grown laugh one woodi comedi year dare say decad never impress scarlet johanson manag tone sexi imag jump right averag spirit young may crown jewel career wittier devil wear prada interest superman great comedi go see friend', ' basic famili littl boy jake think zombi closet parent fight movi slower soap opera suddenli jake decid becom rambo kill first go make film must decid thriller drama drama movi watchabl parent divorc argu like real life jake closet total ruin film expect see boogeyman similar movi instead watch drama meaningless thriller 10 well play parent descent dialog shot jake ignor', ' petter mattei love time money visual stun film watch mattei offer us vivid portrait human relat movi seem tell us money power success peopl differ situat encount variat arthur schnitzler play theme director transfer action present time new york differ charact meet connect one connect one way anoth next person one seem know previou point contact stylishli film sophist luxuri look taken see peopl live world live thing one get soul pictur differ stage loneli one inhabit big citi exactli best place human relat find sincer fulfil one discern case peopl act good mattei direct steve buscemi rosario dawson carol kane michael imperioli adrian grenier rest talent cast make charact come wish mattei good luck await anxious next work']\n"
          ]
        }
      ],
      "metadata": {
        "id": "DyaSkfcvYGXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes classifier"
      ],
      "metadata": {
        "id": "_FkJ-e2pUwun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# get reviews column from df\n",
        "reviews = words2\n",
        "\n",
        "# get labels column from df\n",
        "labels = sent"
      ],
      "outputs": [],
      "metadata": {
        "id": "eVq-mN28U_J4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# Use label encoder to encode labels. Convert to 0/1\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "print(encoded_labels)\n",
        "# print(labels)\n",
        "classes = encoder.classes_\n",
        "print(classes)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 ... 0 0 0]\n",
            "['negative' 'positive']\n"
          ]
        }
      ],
      "metadata": {
        "id": "Ljo5NquhXTXr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# Split the data into train and test (80% - 20%). \n",
        "# Use stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\n",
        "\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels, test_size=0.20) \n",
        "# print(train_sentences.shape, train_labels.shape)\n",
        "print(len(test_sentences), len(train_sentences))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 40000\n"
          ]
        }
      ],
      "metadata": {
        "id": "wzG-C_EVWWET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here there are two approaches possible for building vocabulary for the naive Bayes.\n",
        "1. Take the whole data (train + test) to build the vocab. In this way while testing there is no word which will be out of vocabulary.\n",
        "2. Take the train data to build vocab. In this case, some words from the test set may not be in vocab and hence one needs to perform smoothing so that one the probability term is not zero.\n",
        " \n",
        "You are supposed to go by the 2nd approach.\n",
        " \n",
        "Also building vocab by taking all words in the train set is memory intensive, hence you are required to build vocab by choosing the top 2000 - 3000 frequent words in the training corpus.\n",
        "\n",
        "> $ P(x_i | w_j) = \\frac{ N_{x_i,w_j}\\, +\\, \\alpha }{ N_{w_j}\\, +\\, \\alpha*d} $\n",
        "\n",
        "\n",
        "$N_{x_i,w_j}$ : Number of times feature $x_i$ appears in samples of class $w_j$\n",
        "\n",
        "$N_{w_j}$ : Total count of features in class $w_j$\n",
        "\n",
        "$\\alpha$ : Parameter for additive smoothing. Here consider $\\alpha$ = 1\n",
        "\n",
        "$d$ : Dimentionality of the feature vector  $x = [x_1,x_2,...,x_d]$. In our case its the vocab size.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bz1YdsSkiWCX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Use Count vectorizer to get frequency of the words\n",
        "'''\n",
        "max_features parameter : If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
        "vec = CountVectorizer(max_features = 3000)\n",
        "X = vec.fit_transform(Sentence_list)\n",
        "'''\n",
        "vec = CountVectorizer(max_features = 3000)\n",
        "X = vec.fit_transform(train_sentences)\n",
        "X = X.toarray()\n",
        "vocab = vec.get_feature_names()\n",
        "print(len(vocab))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000\n"
          ]
        }
      ],
      "metadata": {
        "id": "1cllNfGmUr77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# Use laplace smoothing for words in test set not present in vocab of train set\n",
        "def lapSmoothing():\n",
        "    ans = 1/6000\n",
        "    return ans\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "qzRvPjWaWUnm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "# Build the model. Don't use the model from sklearn\n",
        "pos_cnt = [0]*3000\n",
        "neg_cnt = [0]*3000\n",
        "print (\"---------------- Training In Progress --------------------\")\n",
        "for i in range(len(train_labels)):\n",
        "    if(train_labels[i] == 0):\n",
        "        neg_cnt = neg_cnt+X[i]\n",
        "    else:\n",
        "        pos_cnt = pos_cnt+X[i]\n",
        "print (\"---------------- Training Completed --------------------\")\n",
        "# print(pos_cnt)\n",
        "# print(neg_cnt)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- Training In Progress --------------------\n",
            "---------------- Training Completed --------------------\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "# Test the model on test set and report Accuracy\n",
        "\n",
        "vec2 = CountVectorizer(max_features=3000)\n",
        "Y = vec2.fit_transform(test_sentences)\n",
        "Y = Y.toarray()\n",
        "vocab_test = vec2.get_feature_names()\n",
        "pos_prob=[0]*10000\n",
        "neg_prob=[0]*10000\n",
        "pred=[0]*10000\n",
        "for i in range(len(test_sentences)):\n",
        "    for j in range(len(vocab_test)):\n",
        "        if(Y[i][j]>0):\n",
        "            if(vocab_test[j] in vocab):\n",
        "                pos_prob[i]=pos_prob[i]+(pos_cnt[vocab.index(vocab_test[j])]+1)/6000\n",
        "                neg_prob[i]=neg_prob[i]+(neg_cnt[vocab.index(vocab_test[j])]+1)/6000\n",
        "            else:\n",
        "                pos_prob[i]=(pos_prob[i]*6000 + 1)/6000\n",
        "                neg_prob[i]=(neg_prob[i]*6000 + 1)/6000\n",
        "    if(pos_prob[i]>=neg_prob[i]):\n",
        "        pred[i]=1\n",
        "\n",
        "acc=0\n",
        "for i in range(len(pred)):\n",
        "    if(pred[i]==test_labels[i]):\n",
        "        acc=acc+1\n",
        "\n",
        "print(\"Result of Naive Bayes Model:\")\n",
        "print(\"correct predictions = \",acc)\n",
        "print(\"wrong predictions = \",len(pred)-acc)\n",
        "print(\"Test Set Accuracy = \", acc/100,\"%\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result of Naive Bayes Model:\n",
            "correct predictions =  7279\n",
            "wrong predictions =  2721\n",
            "Test Set Accuracy =  72.79 %\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *LSTM* based Classifier\n",
        "\n",
        "Use the above train and test splits."
      ],
      "metadata": {
        "id": "WlNql0acU7sa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# Hyperparameters of the model\n",
        "vocab_size = vocab # choose based on statistics\n",
        "oov_tok = '<OOK>'\n",
        "embedding_dim = 100\n",
        "max_length = 200 # choose based on statistics, for example 150 to 200\n",
        "padding_type='post'\n",
        "trunc_type='post'"
      ],
      "outputs": [],
      "metadata": {
        "id": "SkqnvbUOXoN0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "# tokenize sentences\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# convert train dataset to sequence and pad sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
        "\n",
        "# convert Test dataset to sequence and pad sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
      ],
      "outputs": [],
      "metadata": {
        "id": "UeycEg9nZAOF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# model initialization\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "    keras.layers.Dense(24, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 100)          77000     \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               84480     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 24)                3096      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 164,601\n",
            "Trainable params: 164,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "id": "Mtw3w895ZP39"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "num_epochs = 5\n",
        "\n",
        "history = model.fit(train_padded, train_labels, \n",
        "                    epochs= num_epochs, verbose=1, \n",
        "                    validation_split= 0.1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1125/1125 [==============================] - 160s 138ms/step - loss: 0.4453 - accuracy: 0.7928 - val_loss: 0.3503 - val_accuracy: 0.8495\n",
            "Epoch 2/5\n",
            "1125/1125 [==============================] - 162s 144ms/step - loss: 0.3646 - accuracy: 0.8455 - val_loss: 0.3647 - val_accuracy: 0.8570\n",
            "Epoch 3/5\n",
            "1125/1125 [==============================] - 149s 133ms/step - loss: 0.3428 - accuracy: 0.8551 - val_loss: 0.3458 - val_accuracy: 0.8515\n",
            "Epoch 4/5\n",
            "1125/1125 [==============================] - 147s 131ms/step - loss: 0.3332 - accuracy: 0.8595 - val_loss: 0.3471 - val_accuracy: 0.8500\n",
            "Epoch 5/5\n",
            "1125/1125 [==============================] - 167s 149ms/step - loss: 0.3171 - accuracy: 0.8674 - val_loss: 0.3429 - val_accuracy: 0.8535\n"
          ]
        }
      ],
      "metadata": {
        "id": "skmaDJMnZTzc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "# Calculate accuracy on Test data\n",
        "model.evaluate(test_padded, test_labels)\n",
        "prediction = model.predict(test_padded)\n",
        "\n",
        "# Get probabilities\n",
        "prob = []\n",
        "for i in range(len(prediction)):\n",
        "    prob.append(prediction[i][0])\n",
        "\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "prob_labels = []\n",
        "for i in range(len(prob)):\n",
        "    if(prob[i]>=0.5):\n",
        "        prob_labels.append(1)\n",
        "    else:\n",
        "        prob_labels.append(0)\n",
        "\n",
        "# Accuracy : one can use classification_report from sklearn\n",
        "target_names = ['negative', 'positive']\n",
        "print(classification_report(test_labels, prob_labels,target_names=target_names))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 12s 36ms/step - loss: 0.3495 - accuracy: 0.8552\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.85      0.85      4957\n",
            "    positive       0.85      0.86      0.86      5043\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.86      0.86      0.86     10000\n",
            "weighted avg       0.86      0.86      0.86     10000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "TjEhWEr5Zq7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get predictions for random examples"
      ],
      "metadata": {
        "id": "TIICV-ySOYL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# reviews on which we need to predict\n",
        "sentence = [\"The movie was very touching and heart whelming\", \n",
        "            \"I have never seen a terrible movie like this\", \n",
        "            \"the movie plot is terrible but it had good acting\"]\n",
        "\n",
        "# convert to a sequence\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "# pad the sequence\n",
        "padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
        "\n",
        "# Get probabilities\n",
        "print(\"Probabilities:\")\n",
        "print(model.predict(padded))\n",
        "sentiments = model.predict(padded)\n",
        "sent_labels = []\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "for i in range(3):\n",
        "    if(sentiments[i][0]>=0.5):\n",
        "        sent_labels.append(1)\n",
        "    else:\n",
        "        sent_labels.append(0)\n",
        "for i in range(3):\n",
        "    if(sent_labels[i] == 1):\n",
        "        print('positive')\n",
        "    else:\n",
        "        print('negative')\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities:\n",
            "[[0.66182697]\n",
            " [0.16814485]\n",
            " [0.14222002]]\n",
            "positive\n",
            "negative\n",
            "negative\n"
          ]
        }
      ],
      "metadata": {
        "id": "m2RmfNL3OYL0"
      }
    }
  ]
}